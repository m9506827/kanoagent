"""
é€²éšç‰ˆ Crew - æ”¯æ´æ¯å€‹ Role ç¨ç«‹é…ç½® API/Local Modelï¼Œä¸¦åŒ…å«é‡è©¦æ©Ÿåˆ¶
"""
from crewai import Crew, Process
from agents import (
    SeniorPreSalesConsultantAgent,
    ProductManagerAgent,
    DesignerAgent,
    ArchitectAgent,
    DeveloperAgent,
    ReviewerAgent,
    TechnicalAgent,
)
from tasks.tasks import create_tasks
from config import get_llm_for_role, get_llm_config
from utils.api_logger import get_api_logger
import os
from dotenv import load_dotenv
import requests
import logging
import time

load_dotenv()

# è¨­ç½®æ—¥èªŒï¼ˆä½¿ç”¨çµ±ä¸€çš„æ—¥èªŒç³»çµ±ï¼‰
from utils.logger_config import setup_logger
logger = setup_logger("crew_advanced", logging.INFO)

# åˆå§‹åŒ– API æ—¥èªŒè¨˜éŒ„å™¨
api_logger = get_api_logger()

def check_ollama_available() -> bool:
    """
    æª¢æŸ¥ Ollama æ˜¯å¦å¯ç”¨ï¼ˆåŒ…æ‹¬æœå‹™é‹è¡Œå’Œæ¨¡çµ„å¯å°å…¥ï¼‰
    
    è¿”å› False çš„æƒ…æ³ï¼š
    1. langchain_community æ¨¡çµ„ä¸å¯å°å…¥ï¼ˆæœªå®‰è£ï¼‰
    2. Ollama æœå‹™æœªé‹è¡Œï¼ˆç„¡æ³•é€£æ¥åˆ° http://localhost:11434ï¼‰
    3. ç¶²è·¯é€£æ¥å•é¡Œï¼ˆtimeout æˆ–é€£æ¥è¢«æ‹’çµ•ï¼‰
    
    è©³ç´°èªªæ˜è«‹åƒè€ƒï¼šOLLAMA_SETUP_GUIDE.md
    """
    # æª¢æŸ¥æ¨¡çµ„æ˜¯å¦å¯å°å…¥
    try:
        from langchain_community.llms import Ollama
    except ImportError:
        logger.debug("langchain_community æ¨¡çµ„ä¸å¯ç”¨ï¼ˆå¯èƒ½æœªå®‰è£ï¼špip install langchain-communityï¼‰")
        return False
    
    # æª¢æŸ¥æœå‹™æ˜¯å¦é‹è¡Œ
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            # å¯é¸ï¼šæª¢æŸ¥æ˜¯å¦æœ‰æ¨¡å‹å¯ç”¨
            try:
                models = response.json().get('models', [])
                if models:
                    logger.debug(f"Ollama å¯ç”¨ï¼Œå·²ä¸‹è¼‰ {len(models)} å€‹æ¨¡å‹")
                else:
                    logger.warning("Ollama æœå‹™é‹è¡Œä¸­ï¼Œä½†æœªä¸‹è¼‰ä»»ä½•æ¨¡å‹")
            except:
                pass
            return True
        else:
            logger.debug(f"Ollama æœå‹™è¿”å›éŒ¯èª¤ç‹€æ…‹ç¢¼: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        logger.debug("Ollama æœå‹™ä¸å¯ç”¨ï¼ˆç„¡æ³•é€£æ¥åˆ° localhost:11434ï¼Œè«‹ç¢ºä¿ Ollama å·²å•Ÿå‹•ï¼‰")
        return False
    except requests.exceptions.Timeout:
        logger.debug("Ollama æœå‹™é€£æ¥è¶…æ™‚ï¼ˆè«‹æª¢æŸ¥æœå‹™æ˜¯å¦æ­£å¸¸é‹è¡Œï¼‰")
        return False
    except Exception as e:
        logger.debug(f"Ollama æœå‹™æª¢æŸ¥å¤±æ•—: {e}")
        return False

def create_llm_instance(model_name: str, llm_type: str, agent_name: str = "unknown"):
    """
    æ ¹æ“šæ¨¡å‹åç¨±å’Œé¡å‹å‰µå»º LLM å¯¦ä¾‹
    
    Args:
        model_name: æ¨¡å‹åç¨±ï¼ˆå¦‚ "gemini/gemini-2.0-flash" æˆ– "ollama/llama3.2:3b"ï¼‰
        llm_type: LLM é¡å‹ï¼ˆ"api" æˆ– "local"ï¼‰
        agent_name: Agent åç¨±ï¼ˆç”¨æ–¼æ—¥èªŒè¨˜éŒ„ï¼‰
    
    Returns:
        LLM å¯¦ä¾‹æˆ–æ¨¡å‹åç¨±å­—ä¸²ï¼ˆå¦‚æœ CrewAI æ”¯æ´ï¼‰
    """
    if llm_type == "local":
        # Local model (Ollama)
        if model_name.startswith("ollama/"):
            model_name = model_name.replace("ollama/", "")
        try:
            from langchain_community.llms import Ollama
            # æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code != 200:
                    raise ConnectionError("Ollama æœå‹™ä¸å¯ç”¨")
            except:
                raise ConnectionError("Ollama æœå‹™ä¸å¯ç”¨")
            
            return Ollama(
                model=model_name,
                base_url="http://localhost:11434",
                temperature=0.7,
            )
        except (ImportError, ConnectionError) as e:
            # å¦‚æœç„¡æ³•ä½¿ç”¨ Ollamaï¼Œä¸æ‡‰è©²è¿”å›å­—ä¸²ï¼Œè€Œæ˜¯æ‹‹å‡ºéŒ¯èª¤
            # å› ç‚ºé€™æ‡‰è©²åœ¨ create_kano_crew_advanced ä¸­å·²ç¶“è™•ç†äº†
            raise ValueError(
                f"ç„¡æ³•ä½¿ç”¨ Local model ({model_name})ï¼š{str(e)}\n"
                "è«‹ç¢ºä¿ Ollama å·²å®‰è£ä¸¦é‹è¡Œï¼Œæˆ–å°‡é…ç½®æ”¹ç‚ºä½¿ç”¨ API modelã€‚"
            )
    
    elif llm_type == "api":
        # API model
        if model_name.startswith("gemini/"):
            # Google Gemini
            model = model_name.replace("gemini/", "")
            try:
                from langchain_google_genai import ChatGoogleGenerativeAI
                google_api_key = os.getenv("GOOGLE_API_KEY")
                if not google_api_key:
                    raise ValueError("æœªè¨­å®š GOOGLE_API_KEY")
                llm_instance = ChatGoogleGenerativeAI(
                    model=model,
                    google_api_key=google_api_key,
                    temperature=0.7,
                )
                # è¨˜éŒ„ API å¯¦ä¾‹å‰µå»ºï¼ˆå¯¦éš›èª¿ç”¨æœƒåœ¨ä»»å‹™åŸ·è¡Œæ™‚ç™¼ç”Ÿï¼‰
                api_logger.log_call(
                    agent_name=agent_name,
                    model=model_name,
                    llm_type=llm_type,
                    status="initialized",
                    duration=0,
                )
                return llm_instance
            except ImportError:
                logger.warning(f"ç„¡æ³•å°å…¥ ChatGoogleGenerativeAIï¼Œä½¿ç”¨å­—ä¸²æ ¼å¼: {model_name}")
                # å˜—è©¦ä½¿ç”¨ CrewAI æ”¯æ´çš„æ ¼å¼
                return model  # ç§»é™¤ "gemini/" å‰ç¶´ï¼Œä½¿ç”¨ "gemini-2.0-flash"
        
        elif model_name.startswith("gpt-") or model_name.startswith("openai/"):
            # OpenAI
            model = model_name.replace("openai/", "")
            try:
                from langchain_openai import ChatOpenAI
                openai_api_key = os.getenv("OPENAI_API_KEY")
                if not openai_api_key:
                    raise ValueError("æœªè¨­å®š OPENAI_API_KEY")
                llm_instance = ChatOpenAI(
                    model=model,
                    openai_api_key=openai_api_key,
                    temperature=0.7,
                )
                # è¨˜éŒ„ API å¯¦ä¾‹å‰µå»º
                api_logger.log_call(
                    agent_name=agent_name,
                    model=model_name,
                    llm_type=llm_type,
                    status="initialized",
                    duration=0,
                )
                return llm_instance
            except ImportError:
                logger.warning(f"ç„¡æ³•å°å…¥ ChatOpenAIï¼Œä½¿ç”¨å­—ä¸²æ ¼å¼: {model_name}")
                return model_name
        
        elif model_name.startswith("deepseek/") or model_name.startswith("deepseek-"):
            # DeepSeek APIï¼ˆèˆ‡ OpenAI API å…¼å®¹ï¼‰
            # é‡è¦ï¼šåªç§»é™¤ "deepseek/" å‰ç¶´ï¼Œä¿ç•™å®Œæ•´çš„æ¨¡å‹åç¨±ï¼ˆå¦‚ "deepseek-chat"ï¼‰
            if model_name.startswith("deepseek/"):
                model = model_name.replace("deepseek/", "")  # deepseek/deepseek-chat -> deepseek-chat
            else:
                model = model_name  # deepseek-chat -> deepseek-chatï¼ˆä¿æŒåŸæ¨£ï¼‰
            try:
                # é‡è¦ï¼šæ ¹æ“šæ¸¬è©¦ï¼ŒCrewAI çš„ OpenAICompletion æ”¯æŒ base_url åƒæ•¸
                # ä½†ä¸æœƒå¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œæ‰€ä»¥æˆ‘å€‘ç›´æ¥ä½¿ç”¨ OpenAICompletion
                from crewai.llms.providers.openai.completion import OpenAICompletion
                deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
                if not deepseek_api_key:
                    raise ValueError("æœªè¨­å®š DEEPSEEK_API_KEY")
                
                # ç›´æ¥å‰µå»º OpenAICompletion å¯¦ä¾‹ï¼Œæ˜ç¢ºè¨­ç½® base_url
                # é€™æ¨£å¯ä»¥ç¢ºä¿ CrewAI ä½¿ç”¨æ­£ç¢ºçš„ DeepSeek ç«¯é»
                llm_instance = OpenAICompletion(
                    model=model,
                    api_key=deepseek_api_key,
                    base_url="https://api.deepseek.com/v1",  # DeepSeek API endpoint
                    temperature=0.7,
                )
                
                # é©—è­‰å¯¦ä¾‹é…ç½®
                try:
                    params = llm_instance._get_client_params()
                    logger.info(f"âœ“ DeepSeek LLM å¯¦ä¾‹å·²å‰µå»º: model={model}")
                    logger.info(f"  Client params: base_url={params.get('base_url', 'N/A')}, api_key={params.get('api_key', '')[:20] if params.get('api_key') else 'N/A'}...")
                    
                    # æª¢æŸ¥å¯¦éš›çš„ client
                    if hasattr(llm_instance, 'client'):
                        client = llm_instance.client
                        if hasattr(client, 'base_url'):
                            actual_url = str(client.base_url)
                            logger.info(f"  Client base_url: {actual_url}")
                            if "api.deepseek.com" not in actual_url:
                                logger.error(f"âœ— éŒ¯èª¤ï¼šclient.base_url ä¸æ˜¯ DeepSeek ç«¯é»ï¼å¯¦éš›å€¼: {actual_url}")
                        
                        # æª¢æŸ¥æ¨¡å‹åç¨±
                        if hasattr(llm_instance, 'model'):
                            logger.info(f"  LLM model å±¬æ€§: {llm_instance.model}")
                except Exception as e:
                    logger.warning(f"âš  ç„¡æ³•é©—è­‰ client é…ç½®: {e}")
                
                # è¨˜éŒ„ API å¯¦ä¾‹å‰µå»º
                api_logger.log_call(
                    agent_name=agent_name,
                    model=model_name,
                    llm_type=llm_type,
                    status="initialized",
                    duration=0,
                )
                return llm_instance
            except ImportError:
                # å¦‚æœç„¡æ³•å°å…¥ OpenAICompletionï¼Œå›é€€åˆ° ChatOpenAI
                try:
                    from langchain_openai import ChatOpenAI
                    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
                    if not deepseek_api_key:
                        raise ValueError("æœªè¨­å®š DEEPSEEK_API_KEY")
                    
                    # è¨­ç½®ç’°å¢ƒè®Šæ•¸ï¼ˆä»¥é˜²è¬ä¸€ï¼‰
                    os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"
                    os.environ["OPENAI_API_KEY"] = deepseek_api_key
                    
                    llm_instance = ChatOpenAI(
                        model=model,
                        openai_api_key=deepseek_api_key,
                        base_url="https://api.deepseek.com/v1",
                        temperature=0.7,
                    )
                    
                    api_logger.log_call(
                        agent_name=agent_name,
                        model=model_name,
                        llm_type=llm_type,
                        status="initialized",
                        duration=0,
                    )
                    return llm_instance
                except ImportError:
                    logger.warning(f"ç„¡æ³•å°å…¥ ChatOpenAIï¼ˆç”¨æ–¼ DeepSeekï¼‰ï¼Œä½¿ç”¨å­—ä¸²æ ¼å¼: {model_name}")
                    return model_name
    
    # é è¨­è¿”å›åŸå§‹æ¨¡å‹åç¨±ï¼ˆè®“ CrewAI è‡ªè¡Œè™•ç†ï¼‰
    return model_name

def create_kano_crew_advanced(user_requirements_text: str = None):
    """å‰µå»ºé€šç”¨å‹è»Ÿé«”é–‹ç™¼åœ˜éšŠ - é€²éšé…ç½®
    
    Args:
        user_requirements_text: ç”¨æˆ¶é€šéäº¤äº’å¼å•å·æä¾›çš„éœ€æ±‚æ–‡æœ¬ï¼ˆå¯é¸ï¼‰
    """
    
    # æª¢æŸ¥å¿…è¦çš„ API Key
    google_api_key = os.getenv("GOOGLE_API_KEY")
    openai_api_key = os.getenv("OPENAI_API_KEY")
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
    ollama_available = check_ollama_available()
    
    # æª¢æŸ¥å“ªäº›è§’è‰²éœ€è¦ä½¿ç”¨ DeepSeek API
    roles_using_deepseek = []
    for role_key in ["pre_sales_consultant", "product_manager", "designer", "architect", "developer"]:
        config = get_llm_config(role_key)
        if config["type"] == "api" and config["api_model"].startswith("deepseek/"):
            roles_using_deepseek.append(role_key)
    
    # è¨­ç½®ç’°å¢ƒè®Šæ•¸
    if google_api_key:
        os.environ["GOOGLE_API_KEY"] = google_api_key
    
    # å¦‚æœæœ‰è§’è‰²ä½¿ç”¨ DeepSeek APIï¼Œå¼·åˆ¶è¨­ç½® DeepSeek ç›¸é—œç’°å¢ƒè®Šæ•¸
    if roles_using_deepseek and deepseek_api_key:
        # é‡è¦ï¼šå¿…é ˆåœ¨å‰µå»º LLM å¯¦ä¾‹ä¹‹å‰è¨­ç½®ç’°å¢ƒè®Šæ•¸
        # å¼·åˆ¶è¦†è“‹ä»»ä½•ç¾æœ‰çš„ OPENAI_API_KEY å’Œ OPENAI_API_BASE
        os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"
        os.environ["OPENAI_API_KEY"] = deepseek_api_key
        os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key
        logger.info(f"âœ“ æª¢æ¸¬åˆ°ä½¿ç”¨ DeepSeek API çš„è§’è‰²: {', '.join(roles_using_deepseek)}")
        logger.info(f"âœ“ å·²è¨­ç½® OPENAI_API_BASE = https://api.deepseek.com/v1")
        logger.info(f"âœ“ å·²è¨­ç½® OPENAI_API_KEY = DEEPSEEK_API_KEY")
        # é©—è­‰ç’°å¢ƒè®Šæ•¸è¨­ç½®
        actual_base = os.getenv("OPENAI_API_BASE")
        actual_key_prefix = os.getenv("OPENAI_API_KEY", "")[:15] + "..." if len(os.getenv("OPENAI_API_KEY", "")) > 15 else os.getenv("OPENAI_API_KEY", "")
        logger.info(f"âœ“ é©—è­‰: OPENAI_API_BASE = {actual_base}")
        logger.info(f"âœ“ é©—è­‰: OPENAI_API_KEY = {actual_key_prefix}")
    else:
        # å¦‚æœæ²’æœ‰ä½¿ç”¨ DeepSeekï¼Œå‰‡ä½¿ç”¨åŸæœ¬çš„è¨­ç½®
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        if deepseek_api_key:
            os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key
    
    # ç²å–æ¯å€‹ Role çš„ LLM é…ç½®
    roles = {
        "pre_sales_consultant": "pre_sales_consultant",
        "product_manager": "product_manager",
        "designer": "designer",
        "architect": "architect",
        "developer": "developer",
        "reviewer": "reviewer",
        "technical": "technical",
    }
    
    llm_configs = {}
    for role_key, role_name in roles.items():
        config = get_llm_config(role_key)
        llm_type = config["type"]
        
        # é©—è­‰é…ç½®
        if llm_type == "local" and not ollama_available:
            logger.warning(
                f"{role_name} é…ç½®ç‚ºä½¿ç”¨ local modelï¼Œä½† Ollama ä¸å¯ç”¨ã€‚"
                f"è‡ªå‹•åˆ‡æ›ç‚º API model: {config['api_model']}"
            )
            llm_type = "api"
        
        if llm_type == "api":
            if not google_api_key and not openai_api_key and not deepseek_api_key:
                raise ValueError(
                    f"{role_name} é…ç½®ç‚ºä½¿ç”¨ API modelï¼Œä½†æœªè¨­å®š API Keyã€‚\n"
                    "è«‹åœ¨ .env ä¸­è¨­å®š GOOGLE_API_KEYã€OPENAI_API_KEY æˆ– DEEPSEEK_API_KEY"
                )
            llm_model = config["api_model"]
        else:
            llm_model = config["local_model"]
        
        llm_configs[role_key] = {
            "model": llm_model,
            "type": llm_type,
            "retry_times": config["retry_times"],
            "retry_delay": config["retry_delay"],
        }
    
    # å‰µå»ºæ‰€æœ‰ Agentsï¼ˆä½¿ç”¨å„è‡ªçš„ LLM é…ç½®ï¼‰
    # å¦‚æœ pre_sales_consultant é…ç½®ä¸å­˜åœ¨ï¼Œä½¿ç”¨ product_manager çš„é…ç½®
    if "pre_sales_consultant" not in llm_configs:
        # ä½¿ç”¨ product_manager çš„é…ç½®ä½œç‚ºé è¨­
        pre_sales_config = llm_configs["product_manager"].copy()
        pre_sales_config["model"] = llm_configs["product_manager"]["model"]
    else:
        pre_sales_config = llm_configs["pre_sales_consultant"]
    
    # é‡è¦ï¼šåœ¨å‰µå»º LLM å¯¦ä¾‹ä¹‹å‰ï¼Œç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¨­ç½®ï¼ˆç‰¹åˆ¥æ˜¯ DeepSeekï¼‰
    # CrewAI çš„ OpenAICompletion æœƒåœ¨å‰µå»ºæ™‚è®€å–ç’°å¢ƒè®Šæ•¸
    if roles_using_deepseek and deepseek_api_key:
        # å†æ¬¡ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¨­ç½®ï¼ˆä»¥é˜²åœ¨å‰µå»ºå¯¦ä¾‹æ™‚è¢«è¦†è“‹ï¼‰
        os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"
        os.environ["OPENAI_API_KEY"] = deepseek_api_key
        logger.info("âœ“ åœ¨å‰µå»º LLM å¯¦ä¾‹å‰ï¼Œå·²ç¢ºä¿ç’°å¢ƒè®Šæ•¸è¨­ç½®æ­£ç¢º")
    
    # ç‚ºæ¯å€‹ Agent å‰µå»º LLM å¯¦ä¾‹ï¼ˆè¨˜éŒ„ Agent åç¨±ç”¨æ–¼æ—¥èªŒï¼‰
    pre_sales_llm = create_llm_instance(pre_sales_config["model"], pre_sales_config["type"], "pre_sales_consultant")
    product_manager_llm = create_llm_instance(llm_configs["product_manager"]["model"], llm_configs["product_manager"]["type"], "product_manager")
    designer_llm = create_llm_instance(llm_configs["designer"]["model"], llm_configs["designer"]["type"], "designer")
    architect_llm = create_llm_instance(llm_configs["architect"]["model"], llm_configs["architect"]["type"], "architect")
    developer_llm = create_llm_instance(llm_configs["developer"]["model"], llm_configs["developer"]["type"], "developer")
    reviewer_llm = create_llm_instance(llm_configs["reviewer"]["model"], llm_configs["reviewer"]["type"], "reviewer")
    technical_llm = create_llm_instance(llm_configs["technical"]["model"], llm_configs["technical"]["type"], "technical")
    
    # é‡è¦ï¼šåœ¨å‰µå»º Agent ä¹‹å‰ï¼Œå†æ¬¡ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¨­ç½®
    # å› ç‚º CrewAI çš„ OpenAICompletion æœƒåœ¨ Agent å‰µå»ºæ™‚é‡æ–°è®€å–ç’°å¢ƒè®Šæ•¸
    if roles_using_deepseek and deepseek_api_key:
        os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"
        os.environ["OPENAI_API_KEY"] = deepseek_api_key
        logger.info("âœ“ åœ¨å‰µå»º Agent å‰ï¼Œå·²ç¢ºä¿ç’°å¢ƒè®Šæ•¸è¨­ç½®æ­£ç¢º")
    
    pre_sales_consultant = SeniorPreSalesConsultantAgent(pre_sales_llm)
    product_manager = ProductManagerAgent(product_manager_llm)
    designer = DesignerAgent(designer_llm)
    architect = ArchitectAgent(architect_llm)
    developer = DeveloperAgent(developer_llm)
    reviewer = ReviewerAgent(reviewer_llm)
    technical = TechnicalAgent(technical_llm)
    
    # é¡¯ç¤º LLM é…ç½®
    print("\n" + "="*70)
    print("LLM é…ç½®ï¼ˆæ¯å€‹ Role ç¨ç«‹é…ç½®ï¼‰")
    print("="*70)
    print(f"{'Role':<20} {'Type':<8} {'Model':<35} {'Retry':<10} {'Status':<10}")
    print("-" * 70)
    for role_key, role_name in roles.items():
        config = llm_configs[role_key]
        original_config = get_llm_config(role_key)
        
        # æª¢æŸ¥æ˜¯å¦å› ç‚º Ollama ä¸å¯ç”¨è€Œè‡ªå‹•é™ç´š
        status = ""
        if original_config["type"] == "local" and config["type"] == "api":
            status = "âš ï¸ é™ç´š"
        elif config["type"] == "local":
            status = "âœ“ Local"
        else:
            status = "âœ“ API"
        
        print(
            f"{role_name:<20} "
            f"{config['type'].upper():<8} "
            f"{config['model']:<35} "
            f"{config['retry_times']}x/{config['retry_delay']}s "
            f"{status:<10}"
        )
    print("="*70)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰é™ç´šæƒ…æ³
    downgraded_roles = []
    for role_key, role_name in roles.items():
        original_config = get_llm_config(role_key)
        if original_config["type"] == "local" and llm_configs[role_key]["type"] == "api":
            downgraded_roles.append(role_name)
    
    if downgraded_roles:
        print(f"\nâš ï¸  è­¦å‘Šï¼šä»¥ä¸‹è§’è‰²é…ç½®ç‚º local modelï¼Œä½† Ollama ä¸å¯ç”¨ï¼Œå·²è‡ªå‹•é™ç´šç‚º APIï¼š")
        for role in downgraded_roles:
            print(f"  - {role}")
        print("\nğŸ’¡ è§£æ±ºæ–¹æ¡ˆï¼š")
        print("  1. ç¢ºä¿ Ollama å·²å®‰è£ä¸¦é‹è¡Œï¼šhttps://ollama.ai/")
        print("  2. ç¢ºä¿å·²ä¸‹è¼‰å°æ‡‰çš„æ¨¡å‹ï¼ˆå¦‚ï¼šollama pull gemma3:4bï¼‰")
        print("  3. æˆ–å°‡é€™äº›è§’è‰²çš„é…ç½®æ”¹ç‚ºä½¿ç”¨ API model")
    
    print()
    
    # å‰µå»ºæ‰€æœ‰ä»»å‹™ï¼ˆå‚³éç”¨æˆ¶éœ€æ±‚æ–‡æœ¬ï¼‰
    tasks = create_tasks(
        pre_sales_consultant,
        product_manager,
        designer,
        architect,
        developer,
        reviewer,
        technical,
        user_requirements_text=user_requirements_text,
    )
    
    # å‰µå»º Crewï¼ˆé…ç½®é‡è©¦æ©Ÿåˆ¶ï¼‰
    crew = Crew(
        agents=[
            pre_sales_consultant,
            product_manager,
            designer,
            architect,
            developer,
            reviewer,
            technical,
        ],
        tasks=tasks,
        process=Process.sequential,
        verbose=True,
        # CrewAI å…§å»ºé‡è©¦æ©Ÿåˆ¶ï¼Œä½†æˆ‘å€‘ä¹Ÿå¯ä»¥é€šéç’°å¢ƒè®Šæ•¸é…ç½®
    )
    
    # é¡¯ç¤º API èª¿ç”¨çµ±è¨ˆï¼ˆåŸ·è¡Œå‰ï¼‰
    logger.info("API æ—¥èªŒè¨˜éŒ„å™¨å·²åˆå§‹åŒ–ï¼Œå°‡è¨˜éŒ„æ‰€æœ‰ API èª¿ç”¨")
    logger.info(f"æ—¥èªŒæª”æ¡ˆï¼š{api_logger.log_file}")
    
    return crew

if __name__ == "__main__":
    crew = create_kano_crew_advanced()
    result = crew.kickoff()
    print("\n" + "="*50)
    print("å°ˆæ¡ˆå®Œæˆï¼")
    print("="*50)
    print(result)
